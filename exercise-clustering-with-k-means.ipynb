{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"jupytext":{"cell_metadata_filter":"-all","formats":"ipynb"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1828856,"sourceType":"datasetVersion","datasetId":933090}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Feature Engineering](https://www.kaggle.com/learn/feature-engineering) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/clustering-with-k-means).**\n\n---\n","metadata":{}},{"cell_type":"code","source":"# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.feature_engineering_new.ex4 import *\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\ndef score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        X[colname], _ = X[colname].factorize()\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n\n# Prepare data\ndf = pd.read_csv(\"../input/fe-course-data/ames.csv\")\nX = df.copy()\ny = X.pop(\"SalePrice\")\nfeatures = [\"LotArea\", \"TotalBsmtSF\", \"FirstFlrSF\", \"SecondFlrSF\", \"GrLivArea\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:52:31.667868Z","iopub.execute_input":"2026-02-14T08:52:31.668360Z","iopub.status.idle":"2026-02-14T08:52:31.738033Z","shell.execute_reply.started":"2026-02-14T08:52:31.668323Z","shell.execute_reply":"2026-02-14T08:52:31.736649Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_38/948598237.py:15: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use(\"seaborn-whitegrid\")\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"The k-means algorithm is sensitive to scale. This means we need to be thoughtful about how and whether we rescale our features since we might get very different results depending on our choices. As a rule of thumb, if the features are already directly comparable (like a test result at different times), then you would *not* want to rescale. On the other hand, features that aren't on comparable scales (like height and weight) will usually benefit from rescaling. Sometimes, the choice won't be clear though. In that case, you should try to use common sense, remembering that features with larger values will be weighted more heavily.\n\n# 1) Scaling Features\n\nConsider the following sets of features. For each, decide whether:\n- they definitely should be rescaled,\n- they definitely should *not* be rescaled, or\n- either might be reasonable\n\nFeatures:\n1. `Latitude` and `Longitude` of cities in California\n2. `Lot Area` and `Living Area` of houses in Ames, Iowa\n3. `Number of Doors` and `Horsepower` of a 1989 model car\n\nOnce you've thought about your answers, run the cell below for discussion.","metadata":{}},{"cell_type":"code","source":"X_scaled = pd.DataFrame()\nX_scaled[features] = (X[features] - X[features].mean()) / X[features].std()\nq_1.check()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:52:31.739940Z","iopub.execute_input":"2026-02-14T08:52:31.740351Z","iopub.status.idle":"2026-02-14T08:52:31.760037Z","shell.execute_reply.started":"2026-02-14T08:52:31.740302Z","shell.execute_reply":"2026-02-14T08:52:31.758800Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.3333333333333333, \"interactionType\": 1, \"questionType\": 4, \"questionId\": \"1_Q1\", \"learnToolsVersion\": \"0.3.5\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct: \n\n\n1. No, since rescaling would distort the natural distances described by Latitude and Longitude.\n2. Either choice could be reasonable, but because the living area of a home tends to be more valuable per square foot, it would make sense to rescale these features so that lot area isn't weighted in the clustering out of proportion to its effect on `SalePrice`, if that is what you were trying to predict.\n3. Yes, since these don't have comparable units. Without rescaling, the number of doors in a car (usually 2 or 4) would have negligible weight compared to its horsepower (usually in the hundreds).\n\nWhat you should take away from this is that the decision of whether and how to rescale features is rarely automatic -- it will usually depend on some domain knowledge about your data and what you're trying to predict. Comparing different rescaling schemes through cross-validation can also be helpful. (You might like to check out the `preprocessing` module in scikit-learn for some of the rescaling methods it offers.)","text/markdown":"<span style=\"color:#33cc33\">Correct:</span> \n\n\n1. No, since rescaling would distort the natural distances described by Latitude and Longitude.\n2. Either choice could be reasonable, but because the living area of a home tends to be more valuable per square foot, it would make sense to rescale these features so that lot area isn't weighted in the clustering out of proportion to its effect on `SalePrice`, if that is what you were trying to predict.\n3. Yes, since these don't have comparable units. Without rescaling, the number of doors in a car (usually 2 or 4) would have negligible weight compared to its horsepower (usually in the hundreds).\n\nWhat you should take away from this is that the decision of whether and how to rescale features is rarely automatic -- it will usually depend on some domain knowledge about your data and what you're trying to predict. Comparing different rescaling schemes through cross-validation can also be helpful. (You might like to check out the `preprocessing` module in scikit-learn for some of the rescaling methods it offers.)\n"},"metadata":{}}],"execution_count":36},{"cell_type":"markdown","source":"-------------------------------------------------------------------------------\n\n# 2) Create a Feature of Cluster Labels\n\nCreating a k-means clustering with the following parameters:\n- features: `LotArea`, `TotalBsmtSF`, `FirstFlrSF`, `SecondFlrSF`,`GrLivArea`\n- number of clusters: 10\n- iterations: 10\n\n(This may take a moment to complete.)","metadata":{}},{"cell_type":"code","source":"X = df.copy()\ny = X.pop(\"SalePrice\")\n\n# Define features for clustering\nfeatures = [\"LotArea\", \"TotalBsmtSF\", \"FirstFlrSF\", \"SecondFlrSF\", \"GrLivArea\"]\n\n# Standardize - Create NEW dataframe, don't overwrite X\nX_scaled = X[features].copy()\nX_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n\n# Fit KMeans and add cluster to ORIGINAL X (not X_scaled)\nkmeans = KMeans(n_clusters=10, n_init=10, random_state=0)\nX[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n\n# Check your answer\nq_2.check()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:52:31.761077Z","iopub.execute_input":"2026-02-14T08:52:31.761492Z","iopub.status.idle":"2026-02-14T08:52:31.985067Z","shell.execute_reply.started":"2026-02-14T08:52:31.761460Z","shell.execute_reply":"2026-02-14T08:52:31.982010Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.3333333333333333, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"2_Q2\", \"learnToolsVersion\": \"0.3.5\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"","metadata":{"lines_to_next_cell":0,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_2.hint()\n#q_2.solution()","metadata":{"lines_to_next_cell":0,"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:52:31.985873Z","iopub.execute_input":"2026-02-14T08:52:31.987166Z","iopub.status.idle":"2026-02-14T08:52:31.994765Z","shell.execute_reply.started":"2026-02-14T08:52:31.987133Z","shell.execute_reply":"2026-02-14T08:52:31.991875Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"You can run this cell to see the result of the clustering, if you like.","metadata":{}},{"cell_type":"markdown","source":"And as before, `score_dataset` will score your XGBoost model with this new feature added to training data.","metadata":{}},{"cell_type":"code","source":"score_dataset(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:52:31.996748Z","iopub.execute_input":"2026-02-14T08:52:31.997516Z","iopub.status.idle":"2026-02-14T08:52:33.975812Z","shell.execute_reply.started":"2026-02-14T08:52:31.997459Z","shell.execute_reply":"2026-02-14T08:52:33.974732Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"0.14243771259560514"},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"-------------------------------------------------------------------------------\n\nThe k-means algorithm offers an alternative way of creating features. Instead of labelling each feature with the nearest cluster centroid, it can measure the distance from a point to all the centroids and return those distances as features.\n\n# 3) Cluster-Distance Features\n\nNow add the cluster-distance features to your dataset. You can get these distance features by using the `fit_transform` method of `kmeans` instead of `fit_predict`.","metadata":{}},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=10, n_init=10, random_state=0)\n\n# Create the cluster-distance features using fit_transform\nX_cd = kmeans.fit_transform(X_scaled)\n\n# Label features with correct column names: \"Centroid_0\", \"Centroid_1\", etc.\nX_cd = pd.DataFrame(X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])])\n\n# Check your answer\nq_3.check()","metadata":{"lines_to_next_cell":0,"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:52:33.977006Z","iopub.execute_input":"2026-02-14T08:52:33.977297Z","iopub.status.idle":"2026-02-14T08:52:34.187957Z","shell.execute_reply.started":"2026-02-14T08:52:33.977275Z","shell.execute_reply":"2026-02-14T08:52:34.186658Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.3333333333333333, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"3_Q3\", \"learnToolsVersion\": \"0.3.5\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()","metadata":{"lines_to_next_cell":0,"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:52:34.189123Z","iopub.execute_input":"2026-02-14T08:52:34.189571Z","iopub.status.idle":"2026-02-14T08:52:34.194908Z","shell.execute_reply.started":"2026-02-14T08:52:34.189543Z","shell.execute_reply":"2026-02-14T08:52:34.193664Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"Run this cell to score these new features, if you like.","metadata":{}},{"cell_type":"code","source":"score_dataset(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T08:52:34.196148Z","iopub.execute_input":"2026-02-14T08:52:34.196593Z","iopub.status.idle":"2026-02-14T08:52:36.015114Z","shell.execute_reply.started":"2026-02-14T08:52:34.196537Z","shell.execute_reply":"2026-02-14T08:52:36.012804Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"0.14243771259560514"},"metadata":{}}],"execution_count":42},{"cell_type":"markdown","source":"# Keep Going #\n\n[**Apply principal components analysis**](https://www.kaggle.com/ryanholbrook/principal-component-analysis) to create features from variation in your data.","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/feature-engineering/discussion) to chat with other learners.*","metadata":{}}]}